# -*- coding: utf-8 -*-
"""AbstractScenes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D9mUeeB97xRlTQ-mnBOgq83lUNOdFa5l
"""

# Commented out IPython magic to ensure Python compatibility.
#  %pip install openai requests -q

from google.colab import files
import json, os
from typing import List, Tuple
import time
from openai import OpenAI

# Upload the two files when prompted
uploaded = files.upload()  # select all_caps.json and all.id

CAPS_FILE = next(path for path in uploaded if path.endswith("all_caps.json"))
IDS_FILE = next(path for path in uploaded if path.endswith("all.id"))

# Quick sanity check
with open(CAPS_FILE, "r", encoding="utf-8") as cf, open(IDS_FILE, "r", encoding="utf-8") as idf:
    sample_caps = [json.loads(next(cf).strip()) for _ in range(3)]
    sample_ids = [int(next(idf).strip()) for _ in range(3)]

# Load the dataset (small subset for testing)
captions_with_spans = []
image_ids = []
SUBSET_SIZE = 30
with open(CAPS_FILE, 'r', encoding='utf-8') as f:
    for i, line in enumerate(f):
        if i >= SUBSET_SIZE:
            break
        try:
            data = json.loads(line.strip())
            captions_with_spans.append(data)
        except json.JSONDecodeError as e:
            print(f"Error parsing line {i}: {e}")

with open(IDS_FILE, 'r', encoding='utf-8') as f:
    for i, line in enumerate(f):
        if i >= SUBSET_SIZE:
            break
        image_ids.append(int(line.strip()))

print(f"Loaded {len(captions_with_spans)} captions")
print(f"Loaded {len(image_ids)} image IDs")
print(f"\nFirst 3 examples:")
for i in range(min(3, len(captions_with_spans))):
    caption_text = captions_with_spans[i][0]
    img_id = image_ids[i]
    print(f"  [{i}] Image {img_id}: {caption_text}")

import json, time, requests

# Configure your DeepSeek API key here
# Get this from your professor
DEEPSEEK_API_KEY = "sk-d3d467db7720469290258f4c2eff3352"  # Replace with actual API key
DEEPSEEK_BASE_URL = "https://api.deepseek.com/v1"  # Or the correct DeepSeek API endpoint

# Initialize the client
client = OpenAI(
    api_key=DEEPSEEK_API_KEY,
    base_url=DEEPSEEK_BASE_URL
)

MODEL_NAME = "deepseek-chat"

BASE_PROMPT = f"""
You are translating one-line sentences from a children's visual narrative into **Traditional Chinese**. Follow these rules exactly:

GLOBAL
1) Output **Traditional Chinese only** (繁體): use 著/頂/個 etc. Do **not** use Simplified forms (着/顶/个).
2) End **every sentence** with "。"
3) Preserve names (Mike=麥克, Jenny=珍妮) exactly.
4) Preserve meaning closely; do not add or omit information.

ASPECT / TENSE
5) Do **not** add aspect/progressive markers (了, 過, 正在, 著) unless the English explicitly requires them:
   • If the English uses “is/are ___ing” and clearly indicates an ongoing action, you may use 在/正在.
   • Otherwise, keep aspect neutral.

WORD CHOICES
6) “Up in the air” → translate as **「在空中」** (not 「在天空中」).
7) “Wearing”: use **戴** for accessories (眼鏡、太陽眼鏡、帽子、皇冠), use **穿** for clothing (衣服、外套、鞋等).
8) Animals’ posture verbs:
   • Birds “sit/sits/sitting” → **停在…上** (kid-friendly; do not use 栖/棲息).
   • Cats “sit/sits/sitting/by/near” → **坐在/蹲在/趴在/在…旁邊**; **never** use 停在 for cats.
9) Balls: keep the noun exactly:
   • “ball” → **球** (generic)
   • “soccer ball” → **足球**
10) “Silly [hat/clothing]” → **滑稽的** (e.g., 滑稽的帽子).
11) Emotion tone:
   • “upset” → prefer **不高興** (kid-friendly) or **沮喪** if context suggests sadness.
12) Progressive aspect: when English uses “is V-ing” for an ongoing, dynamic action (e.g., “is kicking”), prefer 正在/在 (e.g., 正在把球踢給…).
13) Articles: do not add 一個 unless English clearly introduces a new, countable item that matters. If the noun is already specific (e.g., “the soccer ball”), omit 一個.
14) Body parts: if both hands are raised, use 雙手 (e.g., 把雙手舉在空中), not 手.

OUTPUT FORMAT
- Output only the translated Chinese sentence (Traditional), nothing else.

"""

def translate_text(text, max_retries=3, delay=1.0):
    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": BASE_PROMPT},
            {
                "role": "user",
                "content": f"Translate exactly:\n{text}\nChinese:",
            },
        ],
        "temperature": 0.1,      # keep close to literal
        "max_tokens": 128,
    }
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json",
    }

    for attempt in range(max_retries):
        try:
            resp = requests.post(
                f"{DEEPSEEK_BASE_URL}/chat/completions",
                headers=headers,
                json=payload,
                timeout=60,
            )
            resp.raise_for_status()
            data = resp.json()
            return data["choices"][0]["message"]["content"].strip()
        except Exception as exc:
            print(f"Attempt {attempt+1}/{max_retries} failed: {exc}")
            if attempt < max_retries - 1:
                time.sleep(delay)
            else:
                return None

# Test translation function
test_text = "Mike kicks a ball to Jenny"
print(f"Testing translation...")
print(f"English: {test_text}")
test_translation = translate_text(test_text)
print(f"Chinese: {test_translation}")

# Translate captions
translated_data = []
failed_indices = []

for i, (caption_data, img_id) in enumerate(zip(captions_with_spans, image_ids)):
    caption_text = caption_data[0]
    original_spans = caption_data[1]  # We'll need to regenerate these for Chinese

    print(f"\n[{i+1}/{len(captions_with_spans)}] Translating: {caption_text}")

    translated_text = translate_text(caption_text)

    if translated_text:
        # For now, we store empty spans - these need to be regenerated from Chinese parse trees
        # In production, you would parse the Chinese sentence and extract spans
        translated_data.append({
            'index': i,
            'image_id': img_id,
            'original_en': caption_text,
            'translated_zh': translated_text,
            'original_spans': original_spans,  # Keep for reference
            'chinese_spans': []  # To be filled after parsing Chinese sentence
        })
        print(f"  ✓ {translated_text}")
    else:
        failed_indices.append(i)
        print(f"  ✗ Translation failed")

    # Small delay to avoid rate limiting
    time.sleep(0.5)

print(f"\n\nTranslation complete!")
print(f"Successfully translated: {len(translated_data)}/{len(captions_with_spans)}")
if failed_indices:
    print(f"Failed indices: {failed_indices}")

# Save translations in JSON format (same structure as all_caps.json)
# Note: Spans are empty and need to be regenerated from Chinese parses
output_file = "all_caps_zh_test.json"
output_ids_file = "all.id_zh_test"
output_details_file = "translation_details.json"

# Save in format compatible with original dataset
with open(output_file, 'w', encoding='utf-8') as f:
    for item in translated_data:
        # Format: ["caption", [[spans]]]
        # Spans are empty for now - need Chinese parser to regenerate
        json_line = [item['translated_zh'], []]  # Empty spans for now
        json.dump(json_line, f, ensure_ascii=False)
        f.write('\n')

# Save image IDs
with open(output_ids_file, 'w', encoding='utf-8') as f:
    for item in translated_data:
        f.write(f"{item['image_id']}\n")

# Save detailed information for reference
with open(output_details_file, 'w', encoding='utf-8') as f:
    json.dump(translated_data, f, ensure_ascii=False, indent=2)

print(f"\nSaved files:")
print(f"  - {output_file}")
print(f"  - {output_ids_file}")
print(f"  - {output_details_file}")

# Download the output files
from google.colab import files

print("\nDownload output files:")
files.download(output_file)
files.download(output_ids_file)
files.download(output_details_file)

# ============================================================
# Robust HanLP constituency span extraction (Traditional Chinese)
# ============================================================

!pip -q install hanlp opencc-python-reimplemented nltk tqdm ujson

import os, json, ujson, re, time, sys, traceback
from pathlib import Path
from tqdm import tqdm
from opencc import OpenCC

import nltk
from nltk import Tree

import hanlp
# Prefer SMALL for speed; fall back to BASE if unavailable
#CON_MODEL = hanlp.pretrained.constituency.CTB9_CON_FULL_TAG_ELECTRA_SMALL
CON_MODEL = hanlp.pretrained.constituency.CTB9_CON_FULL_TAG_ERNIE_GRAM

parser = hanlp.load(CON_MODEL)   # downloads once to cache; then offline

_t2s = OpenCC('t2s')

import hanlp
[k for k in dir(hanlp.pretrained.constituency) if 'CTB9' in k or 'ELECTRA' in k]

from google.colab import files
import json, os
from typing import List, Tuple
import time
from openai import OpenAI

# Upload the two files when prompted
uploaded = files.upload()  # select all_caps.json and all.id

INPUT_JSONL = 'all_caps_zh.jsonl'           # your translated file (["句子", []] per line)
OUTPUT_JSONL = 'all_caps_zh_spans_v2.jsonl' # NEW output file with spans
!pip -q install hanlp opencc-python-reimplemented nltk tqdm ujson
import os, json, ujson, re
_t2s = OpenCC('t2s')
# NEW: load a Chinese tokenizer so we can feed tokens to the parser
TOK_MODEL = hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH
tok = hanlp.load(TOK_MODEL)

def t2s_with_map(trad: str):
    simp = _t2s.convert(trad)
    if len(simp) == len(trad):
        return simp, list(range(len(trad)))
    # handle many-to-one with alignment
    map_s2t = [None] * len(simp)
    sm = SequenceMatcher(a=trad, b=simp)
    for tag, i1, i2, j1, j2 in sm.get_opcodes():
        if tag in ('equal', 'replace'):
            L = min(i2 - i1, j2 - j1)
            for k in range(L):
                map_s2t[j1 + k] = i1 + k
        elif tag == 'insert':
            anchor = i1 - 1 if i1 > 0 else 0
            for sj in range(j1, j2):
                map_s2t[sj] = anchor
        # 'delete' has no direct target chars to map
    last = 0
    for i in range(len(map_s2t)):
        if map_s2t[i] is None:
            map_s2t[i] = last
        else:
            last = map_s2t[i]
    return simp, map_s2t

# --- Tokenize then parse to an NLTK Tree ---
def hanlp_to_tree(text_simplified: str):
    tokens = tok(text_simplified)              # list[str]
    out = parser([tokens])                     # batch of one
    res = out[0] if isinstance(out, list) else out

    if isinstance(res, Tree):
        return res
    if isinstance(res, str):
        return Tree.fromstring(res)
    if isinstance(res, dict):
        for k in ('con', 'tree', 'brackets'):
            if k in res:
                v = res[k]
                if isinstance(v, list):
                    v = v[0]
                if isinstance(v, str):
                    return Tree.fromstring(v)
    s = str(res)
    if '(' in s and ')' in s:
        return Tree.fromstring(s)
    return None

# --- Extract multi-token constituent spans (token indices) ---
def token_spans_from_tree(tree: Tree):
    leaf_positions = list(tree.treepositions('leaves'))
    pos2idx = {pos: i for i, pos in enumerate(leaf_positions)}
    spans = set()
    for node_pos in tree.treepositions():
        node = tree[node_pos]
        if isinstance(node, str):
            continue
        rel_leaves = node.treepositions('leaves')
        if len(rel_leaves) < 2:
            continue
        idxs = [pos2idx[node_pos + rel] for rel in rel_leaves]
        spans.add((min(idxs), max(idxs)))
    return sorted(spans, key=lambda x: (x[0], x[1]))

# --- Align parser tokens to the simplified text to get char offsets ---
def align_tokens_to_text(tokens, text):
    offsets = []
    i = 0
    n = len(text)
    for tok_ in tokens:
        # skip any whitespace
        while i < n and text[i].isspace():
            i += 1
        L = len(tok_)
        j = i  # ALWAYS initialize j so it's defined
        # direct match at pointer?
        if text[i:i+L] != tok_:
            # search forward from i
            found = text.find(tok_, i)
            if found != -1:
                j = found
            else:
                # fallback: keep j = i so we make progress; clamp L if needed
                pass
        start = max(0, min(j, n))
        end = max(start, min(j + L, n))
        offsets.append((start, end))
        i = end
    return offsets

# --- Main: overwrite to a new file ---
def parse_file_overwrite(input_path, output_path):
    if os.path.exists(output_path):
        os.remove(output_path)

    total = sum(1 for _ in open(input_path, 'r', encoding='utf-8'))
    wrote = 0
    with open(input_path, 'r', encoding='utf-8') as fin, \
         open(output_path, 'w', encoding='utf-8') as fout:
        for line in tqdm(fin, total=total, desc='Parsing (HanLP CTB9, overwrite)'):
            line = line.strip()
            if not line:
                continue
            obj = ujson.loads(line)
            sent_trad = obj[0]

            # 1) T->S & map back to Traditional
            sent_simp, s2t = t2s_with_map(sent_trad)

            # 2) Parse
            tree = hanlp_to_tree(sent_simp)
            if tree is None:
                ujson.dump([sent_trad, []], fout, ensure_ascii=False)
                fout.write('\n')
                continue

            # 3) token spans -> char spans
            token_spans = token_spans_from_tree(tree)
            leaves = tree.leaves()
            token_offsets = align_tokens_to_text(leaves, sent_simp)

            char_spans_trad = []
            for s, e in token_spans:
                c_start_s = token_offsets[s][0]
                c_end_s   = token_offsets[e][1] - 1  # inclusive
                c_start_s = max(0, min(c_start_s, len(s2t)-1)) if s2t else 0
                c_end_s   = max(0, min(c_end_s,   len(s2t)-1)) if s2t else 0
                c_start_t = s2t[c_start_s] if s2t else 0
                c_end_t   = s2t[c_end_s]   if s2t else 0
                if c_end_t < c_start_t:
                    c_start_t, c_end_t = c_end_t, c_start_t
                char_spans_trad.append([int(c_start_t), int(c_end_t)])

            ujson.dump([sent_trad, char_spans_trad], fout, ensure_ascii=False)
            fout.write('\n')
            wrote += 1

    print(f'[done] Wrote {wrote} lines with spans to {output_path}')

# --- Sanity test on one sentence before full run ---
test_sent = "麥克把球踢給珍妮。"
ts, _ = t2s_with_map(test_sent)
print('Tokens:', tok(ts))
print('Tree:\n', hanlp_to_tree(ts))

# --- Run full parse (writes NEW file) ---
parse_file_overwrite(INPUT_JSONL, OUTPUT_JSONL)

# Inspect a few lines
!head -n 5 {OUTPUT_JSONL}

# Download the output files
from google.colab import files

print("\nDownload output files:")
files.download("all_caps_zh_spans_v2.jsonl")

from google.colab import files
import json, os
from typing import List, Tuple
import time
from openai import OpenAI

# Upload the two files when prompted
uploaded = files.upload()  # select all_caps.json and all.id

# Colab cell: JSON -> CSV (first 600 rows)
import json
import pandas as pd
import math

# --- config ---
INPUT_JSON = "translation_details.json"
OUTPUT_CSV = "translation_details_first600.csv"

# --- helpers ---
def pick(o, candidates, default=""):
    for c in candidates:
        if c in o and o[c] not in (None, ""):
            return o[c]
    return default

def to_bool(x):
    if isinstance(x, bool):
        return x
    if isinstance(x, (int, float)) and not math.isnan(x):
        return bool(int(x))
    if isinstance(x, str):
        s = x.strip().lower()
        return s in {"1", "true", "t", "yes", "y"}
    return False

# --- load JSON ---
with open(INPUT_JSON, "r", encoding="utf-8") as f:
    data = json.load(f)

# Normalize top-level to a list of records
if isinstance(data, dict):
    # If dict keyed by index/id, keep value objects and try to sort numerically by key or by 'index'
    items = list(data.items())
    def key_fn(item):
        k, v = item
        try:
            return (0, int(k))
        except Exception:
            return (1, v.get("index", float("inf")))
    items.sort(key=key_fn)
    records = [v for _, v in items]
elif isinstance(data, list):
    records = data[:]
    # If there is an 'index' field, sort by it
    if any(isinstance(r, dict) and "index" in r for r in records):
        records.sort(key=lambda r: r.get("index", float("inf")))
else:
    raise ValueError("Unsupported JSON structure: top-level must be list or dict.")

# --- build rows with flexible key mapping ---
rows = []
for o in records:
    if not isinstance(o, dict):
        continue
    en = pick(o, ["english sentence", "english", "original_en", "en", "source_en", "text_en", "original"])
    zh = pick(o, ["chinese sentence", "chinese", "translated_zh", "zh", "target_zh", "text_zh", "translation"])
    needs_corr_raw = pick(o, ["needs correction", "needs_correction", "requires_correction", "needs_review", "flag", "is_incorrect"])
    corr = pick(o, ["correction if applicable", "correction", "corrected_text", "fix", "revised", "correction_zh"])

    rows.append({
        "english sentence": en,
        "chinese sentence": zh,
        "needs correction": to_bool(needs_corr_raw),
        "correction if applicable": corr
    })

# Limit to first 600 sentences
df = pd.DataFrame(rows).iloc[:600].copy()

# Write CSV (UTF-8 BOM to play nice with Excel/Sheets)
df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")

print(f"Saved {len(df)} rows to {OUTPUT_CSV}")
df.head()

# Download the output files
from google.colab import files

print("\nDownload output files:")
files.download("translation_details_first600.csv")